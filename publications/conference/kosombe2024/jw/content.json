{
    "title": "Large Language Models and Health Checkup Report Interpretation: Possibilities and Limitations",
    "authors": [
        {"name": "Jiwon You", "link": "https://dlab-amc.github.io/htmls/members/Jiwon_You.html", "affiliation": " "},
        {"name": "Hangsik Shin", "link": "https://scholar.google.co.kr/citations?user=N1MUdi4AAAAJ&hl=en", "affiliation": " "}
    ],
    "affiliations": [
        {"id": " ", "name": "Department of Convergence Medicine, Asan Medical Center, Brain Korea 21 Project, University of Ulsan College of Medicine"}
    ],
    "links": {
        "paper": "https://drive.google.com/file/d/1WEDwC_Ax-BUusI8UZ4pmztZkIv9EyYDP/view?usp=drive_link",
        "code": "https://github.com/composable-models/llm_multiagent_debate"
    },
    "abstract": "Health checkups are an important means of assessing a patient's health status and detecting potential diseases at an early stage. However, due to the practical constraints of medical persons needing to treat many patients within a limited time, patients may not receive sufficient explanations about their health checkup results. To address this issue, recent research on using artificial intelligence to interpret medical test results has been actively conducted. This study compares and analyzes the accuracy of test result interpretations using five large language models, serving as a crucial preliminary study for developing technology that can explain health checkup results to patients based on their data.",
    "intro": "Health checkups are an important means of assessing a patient's health status and detecting potential diseases at an early stage. However, due to the practical constraints of medical persons needing to treat many patients within a limited time, patients may not receive sufficient explanations about their health checkup results. To address this issue, recent research on using artificial intelligence to interpret medical test results has been actively conducted. This study compares and analyzes the accuracy of test result interpretations using five large language models, serving as a crucial preliminary study for developing technology that can explain health checkup results to patients based on their data.",
    "method": {
        "text": "Existing approaches towards improving language models have primarily focused on improving the performance of a single language generator. We illustrate how we may treat different instances of the same language models as a 'multiagent society', where individual language model generate and critique the language generations of other instances of the language model. We find that the final answer generated after such a procedure is both more factually accurate and solves reasoning questions more accurately. We illustrate below the quantitative difference between multiagent debate and single agent generation on different domains in reasoning and factual validity.",
        "caption": "Multiagent Debate Improves Reasoning and Factual Accuracy. Accuracy of traditional inference and our multi-agent debate over six benchmarks (chess move optimality reported as a normalized score)",
        "image": "",
        "imageLink": "#"
    },
    "results": {
        "text": "Throughout the experiments in the paper, we used a total of 3 language models agents which debate for a total of two arounds, due to computational cost. However, the underlying performance of multiagent debate can be substantially improved by either using more agents or by having more of rounds of debate. Below, we report the accuracy of solving arithmetic expressions as these individual factors are varied.",
        "caption": "(a) Performance with Increased Agents. Arithmetic performance improves as the number of underlying agents involved in debate increases. (b) Performance with Increased Rounds. Arithmetic performance improves as the number of rounds of underlying debate increases.",
        "image": "",
        "imageLink": "#"
    },
    "discussion": {
        "text": "While we mainly study multiagent debate across multiple instances of the same language model, multiagent debate can also be used to combine different language models together. This enables the strengths of one model to enable better performance in another. Below, we illustrate how a combination of both ChatGPT and Bard can be used together to solve a difficult Grade School Math problem.",
        "caption": "Debate Between ChatGPT and Bard. Illustration of debate between different models. While both models generate incorrect responses to the initial GSM8K problem, debate between the models enables them to generate the correct final answer.",
        "image": "img/gpt_bard.png",
        "imageLink": "#"
    },
    "bibtex": "@article{du2023improving,\ntitle={Improving factuality and reasoning in language models through multiagent debate},\nauthor={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},\njournal={arXiv preprint arXiv:2305.14325},\nyear={2023}\n}"
}