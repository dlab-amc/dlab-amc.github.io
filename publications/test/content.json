{
    "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
    "authors": [
        {"name": "Yilun Du", "link": "https://yilundu.github.io/", "affiliation": "1"},
        {"name": "Shuang Li", "link": "https://people.csail.mit.edu/lishuang/", "affiliation": "1"},
        {"name": "Antonio Torralba", "link": "https://groups.csail.mit.edu/vision/torralbalab/", "affiliation": "1"},
        {"name": "Joshua B. Tenenbaum", "link": "https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en", "affiliation": "1"},
        {"name": "Igor Mordatch", "link": "https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en", "affiliation": "2"}
    ],
    "affiliations": [
        {"id": "1", "name": "MIT"},
        {"id": "2", "name": "Google Brain"}
    ],
    "links": {
        "paper": "https://openreview.net/pdf?id=zj7YuTE4t8",
        "poster": "https://github.com/composable-models/llm_multiagent_debate"
    },
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such 'society of minds' approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
    "intro": "건강검진은 정기적인 검진을 통해 수검자의 건강 이상을 사전에 감지하고, 예방적 조치를 취함으로써, 건강을 유지할 수 있도록 돕는 중요한 잠재질병 평가 수단이다. 건강검진 결과는 대개 혈압, 혈당, 콜레스테롤, 간 기능, 신장 기능 등 다양한 생리적 수치를 포함하고 있는데, 결과를 충분히 설명할 수 있는 의료자원 부족으로 인해 정상/비정상 판정과 같이 단순 설명만 제공되는 경우가 빈번하며, 이는 결과적으로 수검자들이 자신의 건강 상태를 충분히 이해하지 못하게 하여 건강 증진 효과를 저하할 수 있다. 이러한 한계를 개선하기 위해, 인공지능을 활용하여 의료진을 대신할 수 있는 검사 결과 해석 기술이 활발히 연구되고 있으며 [1, 2], 특히 대규모 언어 모델(Large Language Model)을 활용하는 기술이 주목받고 있다. 하지만, 대규모 언어 모델의 경우 모델간 성능 편차가 존재하며, 특히 수치 데이터 해석에 있어서는 개선이 필요한 것으로 알려져 있어 지속적인 평가와 검증이 요구된다 [3].\n\n본 연구는 건강검진 결과를 자동으로 해석하고 수검자들에게 제공하기 위한 인공지능 개발에 앞서 건강검진 결과 해석에서 대규모 언어 모델의 가능성을 평가하는 것을 목적으로 한다. 이를 위해 현재까지 출시된 대표적인 대규모 언어 모델들의 건강검진 기록지 해석 성능을 비교평가하고 가능성과 한계를 조망한다.",
    "method": {
        "text": "Existing approaches towards improving language models have primarily focused on improving the performance of a single language generator. We illustrate how we may treat different instances of the same language models as a 'multiagent society', where individual language model generate and critique the language generations of other instances of the language model. We find that the final answer generated after such a procedure is both more factually accurate and solves reasoning questions more accurately. We illustrate below the quantitative difference between multiagent debate and single agent generation on different domains in reasoning and factual validity.",
        "caption": "Multiagent Debate Improves Reasoning and Factual Accuracy. Accuracy of traditional inference and our multi-agent debate over six benchmarks (chess move optimality reported as a normalized score)",
        "image": "img/accuracy_small.png",
        "imageLink": "#"
    },
    "results": {
        "text": "Throughout the experiments in the paper, we used a total of 3 language models agents which debate for a total of two arounds, due to computational cost. However, the underlying performance of multiagent debate can be substantially improved by either using more agents or by having more of rounds of debate. Below, we report the accuracy of solving arithmetic expressions as these individual factors are varied.",
        "caption": "(a) Performance with Increased Agents. Arithmetic performance improves as the number of underlying agents involved in debate increases. (b) Performance with Increased Rounds. Arithmetic performance improves as the number of rounds of underlying debate increases.",
        "image": "img/agent_round.png",
        "imageLink": "#"
    },
    "discussion": {
        "text": "While we mainly study multiagent debate across multiple instances of the same language model, multiagent debate can also be used to combine different language models together. This enables the strengths of one model to enable better performance in another. Below, we illustrate how a combination of both ChatGPT and Bard can be used together to solve a difficult Grade School Math problem.",
        "caption": "Debate Between ChatGPT and Bard. Illustration of debate between different models. While both models generate incorrect responses to the initial GSM8K problem, debate between the models enables them to generate the correct final answer.",
        "image": "img/gpt_bard.png",
        "imageLink": "#"
    }

}